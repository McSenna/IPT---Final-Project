# ğŸ§  Local ChatGPT-Style Web App (Ollama + FastAPI + React)

A **fully local, offline ChatGPT-style web application** powered by a custom **Ollama model (`chain`)**.  
This project provides real-time, streaming AI responses using a FastAPI backend and a minimalist React + Vite frontend.

ğŸš« No cloud APIs  
ğŸ”’ No external network calls  
ğŸ’» Runs 100% on your machine

---

## âœ¨ Features

- 100% **local AI inference** using Ollama
- **Streaming responses** (token-by-token, ChatGPT-style)
- FastAPI backend proxy for security and control
- React + Vite frontend with real-time updates
- Optional local API token authentication
- Full conversation history preserved per request
- Clean, distraction-free UI

---

## ğŸ§© System Architecture

```text
Frontend (React + Vite)
        |
        v
Backend (FastAPI Proxy)
        |
        v
Ollama (Local Model: chain)

Why is this setup? 
- Prevents exposing Ollama directly to the browser
- Enable request validation and access control
- Supports safe response streaming
- Keeps all data local and private

```

## ğŸ“ Project Structure

project-root/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.py          # FastAPI backend
â”‚   â”œâ”€â”€ Modelfile          # Ollama model definition
â”‚   â”œâ”€â”€ requirements.txt  # Python dependencies
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.js
â”‚
â””â”€â”€ README.md


## ğŸ›  Prerequisites
```text
Make sure you have the following installed:

- Python 3.10+
- Node.js 18+
- Ollama (latest version)
  